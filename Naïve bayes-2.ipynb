{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that an employee is a smoker given that he/she uses the health insurance plan is: 0.39999999999999997\n"
     ]
    }
   ],
   "source": [
    "# Given probabilities\n",
    "P_A = 0.70  # Probability that an employee uses the health insurance plan\n",
    "P_B_given_A = 0.40  # Probability that an employee is a smoker given that they use the health insurance plan\n",
    "\n",
    "# Calculate the probability of an employee being a smoker and using the health insurance plan\n",
    "P_A_and_B = P_B_given_A * P_A\n",
    "\n",
    "# Calculate the probability that an employee is a smoker given that they use the health insurance plan using conditional probability formula\n",
    "P_B_given_A = P_A_and_B / P_A\n",
    "\n",
    "print(\"The probability that an employee is a smoker given that he/she uses the health insurance plan is:\", P_B_given_A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bernoulli Naive Bayes:**\n",
    "\n",
    "Assumes that features are binary (Bernoulli, boolean variables), meaning each feature is either present or absent.\n",
    "\n",
    "Commonly used for text classification tasks, where each feature represents the presence or absence of a word in the document (e.g., bag-of-words representation).\n",
    "\n",
    "Suitable for cases where the occurrence of each feature matters but the frequency does not.\n",
    "\n",
    "Often used in spam email detection, sentiment analysis, and document classification.\n",
    "\n",
    "**Multinomial Naive Bayes:**\n",
    "\n",
    "Assumes that features represent counts or frequencies (multinomial distribution), meaning each feature is a discrete count of the number of times it occurs.\n",
    "\n",
    "Also commonly used for text classification tasks but considers the frequency of each feature in addition to its presence or absence.\n",
    "\n",
    "Suitable for cases where the frequency of each feature matters, such as document classification where the frequency of words in the document is important.\n",
    "\n",
    "Often used in text classification tasks, including spam detection, topic classification, and language identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ignoring missing values:** One simple approach is to ignore instances with missing values during training and testing. This means that any instance with missing values is excluded from the dataset before applying the Naive Bayes algorithm. While straightforward, this approach can lead to loss of valuable information, especially if the missing values are not randomly distributed.\n",
    "\n",
    "**Imputation:** Another approach is to impute the missing values with some estimated values. For binary features in Bernoulli Naive Bayes, this might involve imputing the missing values with the mode (most frequent value) of that feature or with a default value (e.g., 0 for absence or 1 for presence). However, imputation can introduce biases if not handled carefully and may not always be appropriate, especially if the missingness is related to the target variable.\n",
    "\n",
    "**Model-based imputation:** Instead of using simple statistics like the mode, more sophisticated imputation methods can be used. For example, one could train a separate Bernoulli Naive Bayes model to predict the missing values based on the non-missing features. This approach utilizes the relationships between features to make more informed imputation decisions.\n",
    "\n",
    "**Treating missingness as a separate category:** Depending on the nature of the data, missing values can sometimes be treated as a separate category or level of the feature. This means creating a new category specifically for missing values and including it as part of the feature set during training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes that the features follow a Gaussian (normal) distribution. It's commonly used for continuous features, where the distribution of each feature is estimated using its mean and variance.\n",
    "\n",
    "In the context of multi-class classification, Gaussian Naive Bayes can be applied by extending the basic binary classification approach of Naive Bayes to handle multiple classes. This is typically done using a \"one-vs-all\" (OvA) or \"one-vs-one\" (OvO) strategy, where the classifier is trained to distinguish between each pair of classes or between each class and the rest (OvA).\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "**One-vs-All (OvA):** For each class, a separate binary classifier is trained to distinguish that class from all other classes. During prediction, the class with the highest probability output by any of the classifiers is assigned as the final prediction.\n",
    "\n",
    "**One-vs-One (OvO):** In this approach, a binary classifier is trained for every pair of classes. During prediction, each classifier \"votes\" for one of the two classes, and the class with the most votes is chosen as the final prediction.\n",
    "\n",
    "Gaussian Naive Bayes can be used in either of these strategies for multi-class classification. It's particularly well-suited when the features follow a Gaussian distribution, and it can perform quite well, especially in cases where the assumptions of the algorithm hold true for the data. However, it's important to note that the assumptions of Gaussian Naive Bayes might not always hold in practice, so it's recommended to assess its performance empirically on the given dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. Assignment:**\n",
    "\n",
    "**Data preparation:**\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "**Results:**\n",
    "\n",
    "Report the following performance metrics for each classifier:\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Precision\n",
    "\n",
    "Recall\n",
    "\n",
    "F1 score\n",
    "\n",
    "**Discussion:**\n",
    "\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8869617393737383\n",
      "Recall: 0.8152389047416673\n",
      "F1 Score: 0.8481249015095276\n",
      "\n",
      "Evaluating Multinomial Naive Bayes:\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision: 0.7393175533565436\n",
      "Recall: 0.7214983911116508\n",
      "F1 Score: 0.7282909724016348\n",
      "\n",
      "Evaluating Gaussian Naive Bayes:\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision: 0.7103733928118492\n",
      "Recall: 0.9569516119239877\n",
      "F1 Score: 0.8130660909542995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# fetch dataset\n",
    "from ucimlrepo import fetch_ucirepo  \n",
    "spambase = fetch_ucirepo(id=94) \n",
    "\n",
    "# data as pandas dataframes\n",
    "\n",
    "X = spambase.data.features \n",
    "y = spambase.data.targets \n",
    "\n",
    "\n",
    "# Initialize classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Evaluate each classifier using 10-fold cross-validation\n",
    "classifiers = {\n",
    "    \"Bernoulli Naive Bayes\": bernoulli_nb,\n",
    "    \"Multinomial Naive Bayes\": multinomial_nb,\n",
    "    \"Gaussian Naive Bayes\": gaussian_nb\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Evaluating {name}:\")\n",
    "    accuracy_scores = cross_val_score(clf, X, y, cv=10, scoring='accuracy')\n",
    "    precision_scores = cross_val_score(clf, X, y, cv=10, scoring='precision')\n",
    "    recall_scores = cross_val_score(clf, X, y, cv=10, scoring='recall')\n",
    "    f1_scores = cross_val_score(clf, X, y, cv=10, scoring='f1')\n",
    "\n",
    "    print(f\"Accuracy: {accuracy_scores.mean()}\")\n",
    "    print(f\"Precision: {precision_scores.mean()}\")\n",
    "    print(f\"Recall: {recall_scores.mean()}\")\n",
    "    print(f\"F1 Score: {f1_scores.mean()}\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "**Performance Comparison:**\n",
    "\n",
    "**Bernoulli Naive Bayes achieved the highest accuracy, precision, and F1 score** among the three variants. It performed well in distinguishing between spam and non-spam emails.\n",
    "\n",
    "Multinomial Naive Bayes had slightly lower accuracy, precision, and F1 score compared to Bernoulli Naive Bayes, but it still performed reasonably well.\n",
    "\n",
    "**Gaussian Naive Bayes had the highest recall**, indicating that it had the fewest false negatives (spam emails classified as non-spam), but it had lower precision and F1 score compared to the other two variants.\n",
    "\n",
    "**Performance Analysis:**\n",
    "\n",
    "Bernoulli Naive Bayes might have performed the best because it is well-suited for binary features like those often encountered in spam detection tasks (e.g., presence or absence of certain words or features).\n",
    "\n",
    "Multinomial Naive Bayes might have slightly lower performance because it assumes features follow a multinomial distribution, which might not perfectly capture the nature of the data in this case.\n",
    "\n",
    "Gaussian Naive Bayes assumes features follow a Gaussian distribution, which might not be the case for the features in this dataset. However, it performed well in recall because it's less sensitive to the specific distribution of the data and focuses more on separating classes based on the overall distribution.\n",
    "\n",
    "**Limitations of Naive Bayes:**\n",
    "\n",
    "**Assumption of Independence:** Naive Bayes assumes that features are conditionally independent given the class label, which might not hold true in real-world datasets.\n",
    "\n",
    "**Sensitivity to Feature Distribution:** Performance can degrade if the features violate the distribution assumptions of the chosen Naive Bayes variant.\n",
    "\n",
    "**Handling of Missing Values:** Naive Bayes doesn't handle missing values well and may require preprocessing to impute or handle missing data appropriately.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Bernoulli Naive Bayes appears to be the most suitable variant for the given spam detection task, based on its overall performance across multiple metrics.\n",
    "\n",
    "Further experimentation with preprocessing techniques, feature engineering, or ensemble methods could potentially improve the performance of the classifiers.\n",
    "\n",
    "It's also worth considering other classification algorithms such as decision trees, random forests, or gradient boosting, to compare their performance with Naive Bayes on this dataset.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
